import json
import os
from openai import OpenAI
from pydantic import BaseModel
from dotenv import load_dotenv
from functions.tools import tools
from functions.dispatcher import call_function
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import pandas as pd
from typing import Literal, List, Dict

# Carrega vari√°veis de ambiente (.env)
load_dotenv()

# Inicializa cliente OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Carrega dados da folha de pagamento
df = pd.read_csv("data/Dados.csv")

# Inicializa a base vetorial com Chroma + embeddings
retriever = Chroma(
    persist_directory="./.chrome_langchain_db",
    embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY")),
).as_retriever()

# Define colunas dispon√≠veis no CSV
cabecalho = df.columns.tolist()

# System prompt: instru√ß√µes fixas para o agente
system_prompt = f"""
Voc√™ √© um agente inteligente que responde d√∫vidas sobre a folha de pagamento de um colaborador individual.
Nunca fale sobre dados de outros colaboradores ou sobre valores m√©dios da empresa.
Sempre responda com base apenas nos dados do colaborador atual.

Essas s√£o as colunas dispon√≠veis na folha de pagamento: {', '.join(cabecalho)}.
Ao usar fun√ß√µes que exigem o nome de uma coluna, use **exatamente** os nomes listados acima. N√£o traduza nem reescreva os nomes das colunas.

Voc√™ pode consultar documentos PDF e textos t√©cnicos (como explica√ß√µes sobre FGTS, PIS, IRRF, CBO etc) que foram carregados na base vetorial.
Se os documentos n√£o forem suficientes, voc√™ pode complementar a resposta com seu conhecimento pr√©vio confi√°vel sobre leis trabalhistas, benef√≠cios e temas relacionados √† folha de pagamento no Brasil.
Evite mencionar valores fixos de impostos, percentuais ou faixas salariais que possam ter mudado, a menos que estejam presentes nos documentos carregados.

**Instru√ß√µes de resposta:**
- Se a resposta for valores em moeda, **responda em reais ou na nota√ß√£o da moeda**
- Se a pergunta do usu√°rio for conceitual (como ‚Äúo que √© FGTS?‚Äù ou ‚Äúcomo funciona o IRRF?‚Äù), **responda de forma completa, clara e explicativa**, utilizando os documentos e seu conhecimento se necess√°rio.
"""

# Modelo unificado para insights
class Insight(BaseModel):
    tipo: Literal["texto", "grafico_barras", "grafico_linha", "grafico_pizza"]
    titulo: str
    conteudo: str | None = None
    dados: List[Dict[str, float]] | None = None
    eixo_x: str | None = None
    eixo_y: str | None = None
    valor_total: float | None = None

# Lista de mensagens com o system prompt inicial
messages = [{"role": "system", "content": system_prompt}]
insights_acumulados = []

# In√≠cio do loop de conversa
while True:
    user_question = input("\nDigite sua pergunta sobre a folha de pagamento (ou 'sair' para encerrar): ")
    if user_question.strip().lower() in {"sair", "exit", "quit"}:
        print("Encerrando o assistente.")
        break

    messages.append({"role": "user", "content": user_question})

    completion = client.chat.completions.create(
        model="gpt-4.1-mini-2025-04-14",
        messages=messages,
        tools=tools,
    )

    assistant_message = completion.choices[0].message
    messages.append(assistant_message)

    print(f"\nTokens usados nesta etapa: {completion.usage.total_tokens} tokens")

    if assistant_message.tool_calls:
        for tool_call in assistant_message.tool_calls:
            name = tool_call.function.name
            try:
                args = json.loads(tool_call.function.arguments)
            except json.JSONDecodeError:
                args = {}

            print(f"\nFun√ß√£o chamada: {name}")
            print("Argumentos:", args)

            try:
                result = call_function(name, args)
            except Exception as e:
                result = {"erro": str(e)}

            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": json.dumps(result, default=str)
            })

        messages.append({
            "role": "user",
            "content": "Responda de forma clara e direta. Evite explica√ß√µes extras se a pergunta for objetiva."
        })

        # Gera√ß√£o paralela do insight estruturado
        try:
            completion_2 = client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=messages,
                response_format=Insight
            )
            insight = completion_2.choices[0].message.parsed
            insights_acumulados.append(insight)

            print("\nüìä Insight estruturado gerado com sucesso!")
            print(f"Tipo: {insight.tipo} | T√≠tulo: {insight.titulo}")

        except Exception as e:
            print("‚ö†Ô∏è Falha ao gerar insight estruturado:", e)

        # Gera√ß√£o da resposta textual para o chat
        final_response = client.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=messages,
        )
        resposta_final = final_response.choices[0].message.content
        messages.append({"role": "assistant", "content": resposta_final})
        print("\nüí¨ Resposta final do agente:\n", resposta_final)

    else:
        print("\nüí¨ Resposta final do agente:\n", assistant_message.content)

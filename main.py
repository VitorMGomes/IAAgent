import json
import os
from openai import OpenAI
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from functions.tools import tools
from functions.dispatcher import call_function
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import pandas as pd

# Carrega vari√°veis de ambiente (.env)
load_dotenv()

# Inicializa cliente OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Carrega dados da folha de pagamento
df = pd.read_csv("data/Dados.csv")

# Inicializa a base vetorial com Chroma + embeddings
retriever = Chroma(
    persist_directory="./.chrome_langchain_db",
    embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
).as_retriever()

# Define colunas dispon√≠veis no CSV
cabecalho = df.columns.tolist()

# System prompt: instru√ß√µes fixas para o agente
system_prompt = f"""
Voc√™ √© um agente inteligente que responde d√∫vidas sobre a folha de pagamento de um colaborador individual.
Nunca fale sobre dados de outros colaboradores ou sobre valores m√©dios da empresa.
Sempre responda com base apenas nos dados do colaborador atual.

Essas s√£o as colunas dispon√≠veis na folha de pagamento: {', '.join(cabecalho)}.
Ao usar fun√ß√µes que exigem o nome de uma coluna, use **exatamente** os nomes listados acima. N√£o traduza nem reescreva os nomes das colunas.

Voc√™ pode consultar documentos PDF e textos t√©cnicos (como explica√ß√µes sobre FGTS, PIS, IRRF, CBO etc) que foram carregados na base vetorial.
Se os documentos n√£o forem suficientes, voc√™ pode complementar a resposta com seu conhecimento pr√©vio confi√°vel sobre leis trabalhistas, benef√≠cios e temas relacionados √† folha de pagamento no Brasil.
Evite mencionar valores fixos de impostos, percentuais ou faixas salariais que possam ter mudado, a menos que estejam presentes nos documentos carregados.

**Instru√ß√µes de resposta:**
- Se a resposta for valores em moeda, **responda em reais ou na nota√ß√£o da moeda**
- Se a pergunta do usu√°rio for conceitual (como ‚Äúo que √© FGTS?‚Äù ou ‚Äúcomo funciona o IRRF?‚Äù), **responda de forma completa, clara e explicativa**, utilizando os documentos e seu conhecimento se necess√°rio.
"""

# Lista de mensagens com o system prompt inicial
messages = [{"role": "system", "content": system_prompt}]

# In√≠cio do loop de conversa
while True:
    user_question = input("\nDigite sua pergunta sobre a folha de pagamento (ou 'sair' para encerrar): ")
    if user_question.strip().lower() in {"sair", "exit", "quit"}:
        print("Encerrando o assistente.")
        break

    # Adiciona a pergunta do usu√°rio ao hist√≥rico
    messages.append({"role": "user", "content": user_question})

    # Chamada principal ao modelo com suporte a ferramentas
    completion = client.chat.completions.create(
        model="gpt-4.1-mini-2025-04-14",
        messages=messages,
        tools=tools,
    )

    assistant_message = completion.choices[0].message
    messages.append(assistant_message)

    # Imprime uso de tokens desta etapa
    print(f"\nTokens usados nesta etapa: {completion.usage.total_tokens} tokens")

    # Verifica se h√° fun√ß√µes a serem chamadas
    if assistant_message.tool_calls:
        for tool_call in assistant_message.tool_calls:
            name = tool_call.function.name
            try:
                args = json.loads(tool_call.function.arguments)
            except json.JSONDecodeError:
                args = {}

            # Log de fun√ß√£o chamada
            print(f"\nFun√ß√£o chamada: {name}")
            print("Argumentos:", args)

            try:
                result = call_function(name, args)
            except Exception as e:
                result = {"erro": str(e)}

            # Adiciona resposta da ferramenta ao hist√≥rico
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": json.dumps(result, default=str)
            })

        # Revis√£o final com clareza
        messages.append({
            "role": "user",
            "content": "Responda de forma clara e direta. Evite explica√ß√µes extras se a pergunta for objetiva."
        })

        class RespostaFinalMelhorada(BaseModel):
            response: str = Field(description="Uma resposta clara e direta com os resultados solicitados.")

        completion_2 = client.beta.chat.completions.parse(
            model="gpt-4.1-mini-2025-04-14",
            messages=messages,
            response_format=RespostaFinalMelhorada
        )

        final_response = completion_2.choices[0].message.parsed

        # Adiciona resposta final ao hist√≥rico
        messages.append({"role": "assistant", "content": final_response.response})

        print("\nüí¨ Resposta final do agente:\n", final_response.response)

        # Imprime uso de tokens da segunda chamada
        print(f"\nTokens usados na resposta final: {completion_2.usage.total_tokens} tokens")

    else:
        print("\nüí¨ Resposta final do agente:\n", assistant_message.content)
